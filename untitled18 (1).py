# -*- coding: utf-8 -*-
"""Untitled18.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iobVMzHOIVGMaA-wkqNmwFLDhFfKEqXW

## **Text Analysis System for AAC Communication Boards**

## **Objective**
The goal of this project is to develop a text-analysis system that processes an  English text (500 words) and classifies it into grammatical categories.

Beyond identifying these key words, the system creates semantic embeddings that allow for deeper analysis and clustering. This workflow will form the foundation of an AAC (Augmentative and Alternative Communication) system, where categorized and clustered words can be used to build efficient communication boards for individuals with communication difficulties

**Install necessary  dependencies  libraries and data sets**
"""

# Install necessary  dependencies  libraries for the rest of the code
!pip install pdfkit
!apt-get install -y wkhtmltopdf
!python -m spacy download en_core_web_md
import spacy
nlp = spacy.load("en_core_web_md")

import os
import re
import nltk
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import pdfkit
import matplotlib.pyplot as plt
import seaborn as sns
import math
from collections import Counter
from sklearn.cluster import KMeans
from IPython.display import display, HTML
from nltk.tokenize import word_tokenize
from nltk import pos_tag
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
from urllib.request import urlopen

# Download necessary NLTK datasets
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

nltk.download('punkt_tab')
nltk.download('averaged_perceptron_tagger_eng')
nltk.download('wordnet')
nltk.download('omw-1.4')

#  Install wkhtmltopdf
!apt-get install -y wkhtmltopdf

#  Install pdfkit Python package
!pip install pdfkit

#Download and Read the Article
article_url = "https://raw.githubusercontent.com/noakraler/english/refs/heads/main/article"
article_text = urlopen(article_url).read().decode('utf-8')

"""**text cleaning**"""

# Text Cleaning
# Define unwanted words that should be removed.
unwanted_words = {"at", "et", "eg", "qol", "asd","al"}


def clean_text(text):
    text = text.lower() #lowercasing
    text = re.sub(r'[^a-z\s]', '', text)  # Remove punctuation and numbers
    words = text.split() #spliting into words
    words = [word for word in words if word not in unwanted_words]  # Remove unwanted words
    return ' '.join(words) #return the eords into a clean text string

#applies the cleaning function to the input test ans stores it
article_text = clean_text(article_text)
article_text

"""

**Text Processing: Tokenization, POS Tagging, WordNet Lemmatization & Singularization**




"""

def get_wordnet_pos(treebank_tag):
    if treebank_tag.startswith('J'):
        return wordnet.ADJ  # Adjective
    elif treebank_tag.startswith('V'):
        return wordnet.VERB # verb
    elif treebank_tag.startswith('N'):
        return wordnet.NOUN #noun
    elif treebank_tag.startswith('R'):
        return wordnet.ADV # Adverb
    else:
        return None # if not recognized, return none

#  Tokenization & POS Tagging
tokens = word_tokenize(article_text)  # Splitting text into individual words
pos_tags = pos_tag(tokens)  # Assigning POS  tags to each token
lemmatizer = WordNetLemmatizer()  # Initialize WordNet Lemmatizer


def lemmatize_words(words, pos_tags):
    lemmatized_words = []   # Create a  list for  lemmatized words
    for word, tag in zip(words, pos_tags):  # Loop through words and their POS tags
        wn_tag = get_wordnet_pos(tag)   # Convert POS tag to WordNet format
        if wn_tag:
            lemmatized_words.append(lemmatizer.lemmatize(word, pos=wn_tag))  # Lemmatize using correct POS
        else:
            lemmatized_words.append(lemmatizer.lemmatize(word))  #  define default lemmatization
    return lemmatized_words

tokens = lemmatize_words(tokens, [tag for _, tag in pos_tags])
word_frequencies = Counter(tokens)  # Count occurrences of each word

"""**Frequency counting**"""

word_frequencies  # Count word occurrences after lemmatization

"""**Syntactic Categorization and Deduplication**"""

def lemmatize_words(words, pos_tags):

    lemmatized_words = []  # Initialize an empty list to store lemmatized words
    for word, tag in zip(words, pos_tags):  # Iterate through words and their POS tags
        wn_tag = get_wordnet_pos(tag)  # Convert NLTK POS tag to WordNet format
        if wn_tag:
            lemmatized_words.append(lemmatizer.lemmatize(word, pos=wn_tag))
            # If POS is known, use it for better lemmatization
        else:
            lemmatized_words.append(lemmatizer.lemmatize(word))
            # If POS is unknown, perform default lemmatization
    return lemmatized_words  # Return the processed list of lemmatized words

# Apply the lemmatization function to all tokens
tokens = lemmatize_words(tokens, [tag for _, tag in pos_tags])
# The 'tokens' list now contains words in their base (lemmatized) form

# Initialize a dictionary to store words based on their grammatical category
categories = {
    "NOUN": [], "VERB": [], "ADJ": [], "ADV": [], "CONNECTIVES": []
}

# Assign words to syntactic categories based on POS tags
for word, tag in pos_tags:
    if tag in ["CC", "IN"]:  # CC: Coordinating Conjunction, IN: Subordinating Conjunction
        categories["CONNECTIVES"].append(word)  # Store connectives
    elif tag.startswith("NN"): # Noun
        categories["NOUN"].append(word)
    elif tag.startswith("VB"):  # Verb
        categories["VERB"].append(word)
    elif tag.startswith("JJ"): # Adjective
        categories["ADJ"].append(word)
    elif tag.startswith("RB"): # Adverb
        categories["ADV"].append(word)


# Remove Duplicates-the goal is to keep words in NOUN category if multiple POS tags exist
unique_categories = {
    "NOUN": set(categories["NOUN"]),  # Convert list of nouns to a set to remove duplicates
    "VERB": set(categories["VERB"]) - set(categories["NOUN"]),   # Remove words that appear in NOUN
    "ADJ": set(categories["ADJ"]) - set(categories["NOUN"]) - set(categories["VERB"]),   # Ensure unique adjectives
    "ADV": set(categories["ADV"]) - set(categories["NOUN"]) - set(categories["VERB"]) - set(categories["ADJ"]),  # Ensure unique adverbs
    "CONNECTIVES": set(categories["CONNECTIVES"]) - set(categories["NOUN"]) - set(categories["VERB"]) - set(categories["ADJ"]) - set(categories["ADV"]),  # Ensure unique connectives
}

# Convert back to lists
categories = {key: list(words) for key, words in unique_categories.items()}

categories #print categories

"""**Frequency Analysis: Extracting and Displaying Top Frequent Words by Category**"""

# Frequency Analysis
def get_most_frequent_words(category_words, top_n=12): #this function extracts the top 12 most frequent words from a given category
    return sorted([(word, word_frequencies[word]) for word in category_words if word in word_frequencies], key=lambda x: x[1], reverse=True)[:top_n] #sort words by frequency and keep to top 12

# Apply the function to each syntactic category and store the results in a dictionary
frequent_words = {cat: get_most_frequent_words(words, top_n=12) for cat, words in categories.items()}

# Display frequency counts for each category
for category, words in frequent_words.items():  # Iterate through each category and its words
    print(f"{category} ({len(words)} words): {', '.join([f'{word} ({count})' for word, count in words])}\n")  # Print category name, number of words selected, and their frequency

"""**Visualization of Word Frequency**



"""

# Visualization of Word Frequency
category_colors = {  # Define colors for each grammatical category
    "NOUN": "lightcoral",
    "VERB": "lightblue",
    "ADJ": "lightgreen",
    "ADV": "lightpink",
    "CONNECTIVES": "lightyellow"
}

# Dynamically determine the number of rows needed
num_categories = len(frequent_words)  # Count the number of grammatical categories
rows = math.ceil(num_categories / 2)  # Ensure enough rows for a 2-column layout

# Create a figure with multiple subplots
fig, axes = plt.subplots(nrows=rows, ncols=2, figsize=(14, 4 * rows))  # Define subplot grid
axes = axes.flatten()  # Flatten the array of axes to simplify indexing

# Loop through each category and create a bar chart
for idx, (category, words) in enumerate(frequent_words.items()):
    word_labels, counts = zip(*words)  # Unpack words and their frequencies

    ax = axes[idx]  # Select the correct subplot
    sns.barplot(x=list(counts), y=list(word_labels), ax=ax, color=category_colors[category])  # Create bar plot
    ax.set_title(f"Top Frequent Words in {category}", fontsize=14)  # Set the title
    ax.set_xlabel("Frequency", fontsize=12)  # Label the x-axis
    ax.set_ylabel("Words", fontsize=12)  # Label the y-axis

# Turn off any unused subplots if there are fewer categories than subplots
for idx in range(num_categories, len(axes)):
    fig.delaxes(axes[idx])  # Remove unused subplot spaces

plt.tight_layout()  # Adjust layout to prevent overlap
plt.show()  # Display the plot

"""**Visualizing Category Distribution**"""

# Pie Chart for Category Distribution
fig, ax = plt.subplots(figsize=(8, 8))  #  this line create a figure and axis for the pie chart with a fixed size

category_sizes = {cat: sum([count for _, count in words]) for cat, words in frequent_words.items() if words}
# this create a dictionary where the key is the category, and the value is the sum of word frequencies in that category

labels, sizes = zip(*category_sizes.items())  #  this line extract labels (categories) and sizes (total frequencies) from the dictionary

ax.pie(sizes, labels=labels, autopct='%1.1f%%',   #  this line assigns category names to each slice and displays percentage values on the chart
       colors=[category_colors[cat] for cat in labels], startangle=140)  #  this line assigns colors to categories and rotates the pie chart by 140 degree

"""**Semantic Clustering**"""

# Loads the "en_core_web_md" SpaCy model

def cluster_words(words, word_frequencies):
    """
    Clusters exactly 12 words into 3 clusters (4 words per cluster),
    ensuring words within each cluster are sorted by frequency.
    """
    n_clusters = 3  # Defines the number of clusters to be 3
    if len(words) != 12:
        raise ValueError("The input must contain exactly 12 words.") # make sure we always have exactly 12 words

    #  word embeddings
    valid_words = [word for word in words if nlp(word).vector_norm > 0]  # Filters out words without valid embeddings
    vectors = np.array([nlp(word).vector for word in valid_words])

    # Apply KMeans clustering
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    labels = kmeans.fit_predict(vectors)  #gets cluster labels for each word

    # Assign words to clusters
    clustered_words = {i: [] for i in range(n_clusters)}  # make a dictionary to store words in each cluster
    for word, cluster in zip(valid_words, labels):
        clustered_words[cluster].append(word)  #Placing every to its assigned cluster

    # Ensure each cluster has exactly 4 words
    sorted_clusters = sorted(clustered_words.values(), key=len, reverse=True)  # Sorts clusters by size (largest first)
    flattened = sum(sorted_clusters, [])   # move all clusters into one list

    final_clusters = {i: flattened[i*4:(i+1)*4] for i in range(n_clusters)}

    # Sort words within each cluster by frequency (highest first)
    for cluster_id in final_clusters:
        final_clusters[cluster_id] = sorted(final_clusters[cluster_id],
                                           key=lambda w: word_frequencies[w],
                                           reverse=True)

    return final_clusters

# Apply clustering to all categories, using the top 12 frequent words
clustered_words = {
    cat: cluster_words([word for word, _ in words[:12]], word_frequencies)
    for cat, words in frequent_words.items()
}

# Print final clusters
for category, clusters in clustered_words.items():
    print(f"\nCategory: {category}")
    for cluster_id, words in clusters.items():
        print(f"  Cluster {cluster_id}: {', '.join(words)}")

"""**Building the AAC table**"""

# Define category colors and assign a specific color for each one of them
category_colors = {
    "NOUN": "#FFDDC1",  # NOUN category
    "VERB": "#C1FFD7",  #  VERB category
    "ADJ": "#C1D9FF",  # adjective category
    "ADV": "#FFD1DC",   # adverb category
    "CONNECTIVES": "#E2C1FF"    #  CONNECTIVES category
}

#  building the HTML table
html_output = """
<html>
<head>
<style>
    table {border-collapse: collapse; width: 100%;}  /* Set table to collapse borders and span full width */
    th, td {border: 1px solid black; padding: 8px; text-align: center;}  /* Style table headers and cells */
    th {background-color: #f2f2f2;}  /* Set background color for table headers */
    .category-noun {background-color: #FFDDC1;}  /* CSS class for NOUN category styling */
    .category-verb {background-color: #C1FFD7;}  /* CSS class for VERB category styling */
    .category-adj {background-color: #C1D9FF;}   /* CSS class for ADJ category styling */
    .category-adv {background-color: #FFD1DC;}   /* CSS class for ADV category styling */
    .category-connectives {background-color: #E2C1FF;}  /* CSS class for CONNECTIVES category styling */
</style>
</head>
<body>
<table>
<tr><th colspan='18'>AAC </th></tr>
"""

for category, clusters in clustered_words.items():  # Loop through each syntactic category and its clusters
    css_class = f"category-{category.lower()}"  # Generate a CSS class name based on the lowercase category name

    html_output += f"<tr><th colspan='18' class='{css_class}'>{category}</th></tr><tr>"      # Add category headlines


    # Loop through the 3 clusters
    for cluster_id, words in clusters.items():  # Loop through each cluster within the current category
        for word in words:  # Loop through each word in the current cluster
            html_output += f"<td class='{css_class}'>{word}</td>"
            # Add a table cell for each word with the corresponding CSS class

        # Add thick border to separate clusters visually
        html_output += "<td style='border-right: 3px solid black;'></td>"
        # Insert an extra cell with a thick right border to visually separate clusters

    html_output += "</tr>"  # Close the current row for the category

html_output += "</table></body></html>"  # Complete the HTML by closing the table, body, and html tags

# Display the table
display(HTML(html_output))  # Render and display the constructed HTML table in the notebook

"""**Converting to a PDF file and downloading it**"""

from google.colab import files # Import the files module from google.colab

html_table = html_output   # Assigns the HTML string (stored in 'html_output') to 'html_table'.

# Opens 'output.html' in write mode and writes the HTML string into it.
with open("output.html", "w", encoding="utf-8") as f:
    f.write(html_table)

config = pdfkit.configuration(wkhtmltopdf="/usr/bin/wkhtmltopdf")  # Configures pdfkit to use the wkhtmltopdf binary located at '/usr/bin/wkhtmltopdf'.

pdfkit.from_file("output.html", "output.pdf", configuration=config) # Converts 'output.html' to 'output.pdf' using the specified configuration.
print("PDF exported as output.pdf") # Prints a confirmation message

files.download("output.pdf") # Initiates download of 'output.pdf' in Google Colab.